{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from chronos import ChronosPipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(path, ticker):\n",
    "    try:\n",
    "        pipeline = ChronosPipeline.from_pretrained(\n",
    "            \"amazon/chronos-t5-small\", # Use tiny\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "    except ImportError as e:\n",
    "        logging.error(f\"❌ Error loading the Chronos model for {ticker}: {e}\")\n",
    "        raise ImportError(f\"❌ Error loading the Chronos model for {ticker}: {e}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_parquet(path)\n",
    "        context = torch.tensor(df[\"Close\"].values)\n",
    "        embeddings, tokenizer_state = pipeline.embed(context)\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"❌ Error extracting embeddings for {ticker}: {e}\")\n",
    "        raise ValueError(f\"❌ Error extracting embeddings for {ticker}: {e}\")\n",
    "    \n",
    "    logging.info(f\"✅ Successfully extracted embeddings for {ticker}.\")\n",
    "    return embeddings, tokenizer_state\n",
    "\n",
    "# Use Ilyaas function\n",
    "def extract_all_embeddings(data_dir):\n",
    "    try:\n",
    "        data_dir = Path(data_dir)\n",
    "        files = list(data_dir.glob(\"*.parquet\"))\n",
    "    except FileExistsError as e:\n",
    "        logging.error(f\"❌ Directory {data_dir} not found, {e}.\")\n",
    "        raise FileExistsError(f\"❌ Directory {data_dir} not found, {e}.\")\n",
    "    \n",
    "    tickers = []\n",
    "    embedding_list = []\n",
    "    \n",
    "    for file_path in files:\n",
    "        ticker = file_path.stem\n",
    "        embeddings, _ = get_embeddings(file_path, ticker)\n",
    "\n",
    "        # If embeddings has a singleton batch dimension (e.g., shape (1, T, D)), squeeze it.\n",
    "        if embeddings.ndim == 3 and embeddings.shape[0] == 1:\n",
    "            embeddings = embeddings.squeeze(0)  # Now shape becomes (T, D)\n",
    "        \n",
    "        # If embeddings are time-distributed (i.e. shape (T, D)), average over time axis.\n",
    "        if embeddings.ndim == 2:\n",
    "            embedding_vector = embeddings.mean(dim=0).float().numpy()\n",
    "        else:\n",
    "            embedding_vector = embeddings.float().numpy()\n",
    "\n",
    "        tickers.append(ticker)\n",
    "        embedding_list.append(embedding_vector)\n",
    "    \n",
    "    logging.info(f\"✅ Successfully extracted all embeddings for time series in {data_dir}.\")\n",
    "    return tickers, np.array(embedding_list)\n",
    "\n",
    "\n",
    "def cluster_embeddings(data_dir, n_clusters=5):\n",
    "    tickers, embeddings_array = extract_all_embeddings(data_dir)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(embeddings_array)\n",
    "\n",
    "    ticker_cluster_dict = {ticker: cluster for ticker, cluster in zip(tickers, clusters)}\n",
    "    return ticker_cluster_dict\n",
    "\n",
    "\n",
    "def save_stocks_to_cluster_dirs(clusters, data_directory, output_directory):\n",
    "    try:\n",
    "        cluster_dirs = {cluster: os.path.join(output_directory, f\"cluster_{cluster}\")\n",
    "                        for cluster in set(clusters.values())}\n",
    "        for path in cluster_dirs.values():\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        for stock, cluster in clusters.items():\n",
    "            source_file = os.path.join(data_directory, f\"{stock}.parquet\")\n",
    "            dest_file = os.path.join(cluster_dirs[cluster], f\"{stock}.parquet\")\n",
    "            if os.path.exists(source_file):\n",
    "                shutil.copy(source_file, dest_file)\n",
    "                logging.info(f\"Copied {stock}.parquet to cluster_{cluster} directory.\")\n",
    "            else:\n",
    "                logging.warning(f\"❌ File {source_file} not found!\")\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"❌ Directory {data_directory} not found: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_directory = \"/Users/akramchakrouni/Projects/time-series-forecasting-cluserting/data/chronos/\"\n",
    "    output_directory = \"/Users/akramchakrouni/Projects/time-series-forecasting-cluserting/clusters/embeddings\"\n",
    "    n_clusters = 5  # Adjust based on your analysis\n",
    "    clusters_dict = cluster_embeddings(data_directory, n_clusters)\n",
    "    save_stocks_to_cluster_dirs(clusters_dict, data_directory, output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eosl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
