{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.timeseries import TimeSeriesPredictor\n",
    "import os\n",
    "\n",
    "# Dictionary mapping each cluster to a list of model directories (one model per time series)\n",
    "cluster_models = {\n",
    "    'cluster_1': ['models/cluster_1_model1', 'models/cluster_1_model2'],\n",
    "    'cluster_2': ['models/cluster_2_model1', 'models/cluster_2_model2'],\n",
    "    # add additional clusters and models as needed\n",
    "}\n",
    "\n",
    "# File paths (update these as appropriate for your environment)\n",
    "ACTUALS_PATH = 'data/weekly_actuals.csv'        # CSV with columns: cluster_id, timestamp, value\n",
    "PREDICTIONS_PATH = 'data/weekly_predictions.csv'  # CSV with columns: cluster_id, timestamp, value, model_name (optional)\n",
    "ALL_DATA_PATH = 'data/all_cluster_data.csv'       # Historical data used for fine-tuning\n",
    "\n",
    "# Define the error threshold (e.g., if MAPE > 10% fine-tune the model)\n",
    "ERROR_THRESHOLD = 0.10\n",
    "\n",
    "def calculate_mape(actual, predicted):\n",
    "    \"\"\"Calculate the Mean Absolute Percentage Error (MAPE).\"\"\"\n",
    "    actual = np.where(actual == 0, 1e-8, actual)\n",
    "    return np.mean(np.abs((actual - predicted) / actual))\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"Load CSV file data into a DataFrame.\"\"\"\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def fine_tune_model(model_dir, training_data):\n",
    "    \"\"\"\n",
    "    Fine tune the AutoGluon TimeSeries model saved in model_dir.\n",
    "    Training_data should include the new weekâ€™s actual values along with historical data.\n",
    "    \"\"\"\n",
    "    # Load the pre-trained or previously fine tuned model\n",
    "    predictor = TimeSeriesPredictor.load(model_dir)\n",
    "    \n",
    "    # Fine tune using new data with fine_tune flag enabled.\n",
    "    predictor.fit(\n",
    "        training_data,\n",
    "        hyperparameters={\"Chronos\": {\"fine_tune\": True}},\n",
    "        time_limit=600  # Adjust as necessary\n",
    "    )\n",
    "    \n",
    "    # Save the updated model\n",
    "    predictor.save(model_dir)\n",
    "    return predictor\n",
    "\n",
    "def process_model(cluster_id, model_dir, actual_df, pred_df, all_data_df):\n",
    "    \"\"\"\n",
    "    Process a single model:\n",
    "      - Compute error by comparing last week's prediction with actual values.\n",
    "      - Fine tune if error exceeds the threshold.\n",
    "    \"\"\"\n",
    "    # Filter the actual and prediction data for the given cluster\n",
    "    cluster_actual = actual_df[actual_df['cluster_id'] == cluster_id].copy()\n",
    "    cluster_pred = pred_df[ (pred_df['cluster_id'] == cluster_id) & (pred_df['model_dir'] == model_dir) ].copy()\n",
    "    \n",
    "    # Optionally, sort by timestamp to ensure alignment (assumes same order)\n",
    "    cluster_actual.sort_values('timestamp', inplace=True)\n",
    "    cluster_pred.sort_values('timestamp', inplace=True)\n",
    "    \n",
    "    # Calculate error metric (MAPE)\n",
    "    error = calculate_mape(cluster_actual['value'].values, cluster_pred['value'].values)\n",
    "    print(f\"Model {model_dir} for cluster {cluster_id} MAPE: {error:.2%}\")\n",
    "    \n",
    "    # Only fine tune if the error exceeds the threshold\n",
    "    if error > ERROR_THRESHOLD:\n",
    "        print(f\"Error exceeds threshold for model {model_dir} in cluster {cluster_id}. Fine tuning...\")\n",
    "        # Gather training data for this cluster\n",
    "        cluster_training_data = all_data_df[all_data_df['cluster_id'] == cluster_id]\n",
    "        # Fine tune the model\n",
    "        fine_tuned_model = fine_tune_model(model_dir, cluster_training_data)\n",
    "        print(f\"Model {model_dir} for cluster {cluster_id} has been fine tuned and updated.\")\n",
    "    else:\n",
    "        print(f\"Model {model_dir} for cluster {cluster_id} is within acceptable error limits.\")\n",
    "\n",
    "def main():\n",
    "    # Load actuals, predictions, and historical (all) data from CSV files\n",
    "    actual_df = load_data(ACTUALS_PATH)\n",
    "    pred_df = load_data(PREDICTIONS_PATH)\n",
    "    all_data_df = load_data(ALL_DATA_PATH)\n",
    "    \n",
    "    # Optionally, add a 'model_dir' column in the predictions CSV if not already present.\n",
    "    # This column should indicate which model produced the prediction.\n",
    "    # For example, you might have saved the model identifier during prediction.\n",
    "    \n",
    "    # Loop over each cluster and each model in that cluster\n",
    "    for cluster_id, models in cluster_models.items():\n",
    "        for model_dir in models:\n",
    "            process_model(cluster_id, model_dir, actual_df, pred_df, all_data_df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
